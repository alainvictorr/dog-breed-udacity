{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# load filenames for human and dog images\n",
    "human_files = np.array(glob(\"./data/lfw/*/*\"))\n",
    "dog_files = np.array(glob(\"./data/dogImages/*/*/*\"))\n",
    "\n",
    "# print number of images in each dataset\n",
    "print('There are %d total human images.' % len(human_files))\n",
    "print('There are %d total dog images.' % len(dog_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# define VGG16 model\n",
    "VGG16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# move model to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    VGG16 = VGG16.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(VGG16):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16.classifier[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def VGG16_predict(img_path):\n",
    "    '''\n",
    "    Use pre-trained VGG-16 model to obtain index corresponding to \n",
    "    predicted ImageNet class for image at specified path\n",
    "    \n",
    "    Args:\n",
    "        img_path: path to an image\n",
    "        \n",
    "    Returns:\n",
    "        Index corresponding to VGG-16 model's prediction\n",
    "    '''\n",
    "    \n",
    "    ## TODO: Complete the function.\n",
    "    ## Load and pre-process an image from the given img_path\n",
    "    img = load_image(img_path)\n",
    "    if use_cuda:\n",
    "        img = img.cuda()\n",
    "    ret = VGG16(img) \n",
    "    \n",
    "    ## Return the *index* of the predicted class for that image\n",
    "        \n",
    "    return torch.max(ret,1)[1].item() # predicted class index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "pretrained_size = 224\n",
    "pretrained_means = [0.485, 0.456, 0.406]\n",
    "pretrained_stds= [0.229, 0.224, 0.225]\n",
    "\n",
    "def load_image(img_path):    \n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    # resize to (244, 244) because VGG16 accept this shape\n",
    "    train_transforms = transforms.Compose([\n",
    "                           transforms.Resize(pretrained_size),\n",
    "                           transforms.RandomRotation(5),\n",
    "                           transforms.RandomHorizontalFlip(0.5),\n",
    "                           transforms.RandomCrop(pretrained_size, padding = 10),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean = pretrained_means, \n",
    "                                                std = pretrained_stds)\n",
    "                       ])\n",
    "    \n",
    "    test_transforms = transforms.Compose([\n",
    "                           transforms.Resize(pretrained_size),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean = pretrained_means, \n",
    "                                                std = pretrained_stds)\n",
    "                       ])\n",
    "\n",
    "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
    "    image = train_transforms(image)[:3,:,:].unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    ## TODO: Complete the function.\n",
    "    value=VGG16_predict(img_path)\n",
    "    if 268>=value>=151:\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "    #return None # true/false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2='./Data/dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg'\n",
    "url3='./Data/dogImages/test/006.American_eskimo_dog/American_eskimo_dog_00406.jpg'\n",
    "url4='./Data/lfw/Aaron_Guiel/Aaron_Guiel_0001.jpg'\n",
    "\n",
    "print(dog_detector(url2))\n",
    "print(dog_detector(url3))\n",
    "print(dog_detector(url4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "human_files_short = human_files[:100]\n",
    "dog_files_short = dog_files[:100]\n",
    "\n",
    "count=0\n",
    "count2=0\n",
    "for i in tqdm(range(len(human_files_short))):\n",
    "  if dog_detector(human_files_short[i])==True:\n",
    "    count+=1\n",
    "  if dog_detector(dog_files_short[i])==True:\n",
    "    count2+=1\n",
    "\n",
    "print('')\n",
    "print(count/len(human_files_short),count2/len(dog_files_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "#seed_value = 1234\n",
    "seed_value = 1000\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "### TODO: Write data loaders for training, validation, and test sets\n",
    "## Specify appropriate transforms, and batch_sizes\n",
    "\n",
    "pretrained_means = [0.485, 0.456, 0.406]\n",
    "pretrained_stds= [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                           transforms.Resize(256),\n",
    "                           transforms.RandomPerspective(0.5,0.5,2,0),\n",
    "                           transforms.RandomHorizontalFlip(),\n",
    "                           transforms.CenterCrop(224),\n",
    "                           transforms.ToTensor(), \n",
    "                           transforms.Normalize(mean = pretrained_means, \n",
    "                                                std = pretrained_stds)\n",
    "                       ])\n",
    "\n",
    "valid_transforms = transforms.Compose([\n",
    "                           #transforms.Resize(pretrained_size),#test orig out\n",
    "                           transforms.Resize(256), #test orig\n",
    "                           transforms.CenterCrop(224),#test orig \n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean = pretrained_means, \n",
    "                                                std = pretrained_stds)\n",
    "                       ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                           #transforms.Resize(size=(pretrained_size,pretrained_size)),\n",
    "                           transforms.Resize(256),\n",
    "                           transforms.CenterCrop(224), #test orig\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean = pretrained_means, \n",
    "                                                std = pretrained_stds)\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=datasets.ImageFolder('./Data/dogImages/train',transform=train_transforms)\n",
    "valid_data=datasets.ImageFolder('./Data/dogImages/valid',transform=valid_transforms)\n",
    "test_data=datasets.ImageFolder('./Data/dogImages/test',transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 6680\n",
      "Number of validation examples: 835\n",
      "Number of testing examples: 836\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "train_iterator = data.DataLoader(train_data, \n",
    "                                 shuffle = True, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "valid_iterator = data.DataLoader(valid_data, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "test_iterator = data.DataLoader(test_data, \n",
    "                                batch_size = BATCH_SIZE)\n",
    "\n",
    "loaders= {\n",
    "    'train': train_iterator,\n",
    "    'valid': valid_iterator,\n",
    "    'test': test_iterator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_classes = 133\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## Define layers of a CNN\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "\n",
    "        # pool\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # fully-connected\n",
    "        self.fc1 = nn.Linear(7*7*128, 500)\n",
    "        self.fc2 = nn.Linear(500, num_classes) \n",
    "        \n",
    "        # drop-out\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(-1, 7*7*128)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder and decoder to convert classes into integer\n",
    "classes = train_data.classes\n",
    "decoder = {}\n",
    "for i in range(len(classes)):\n",
    "    decoder[classes[i]] = i\n",
    "encoder = {}\n",
    "for i in range(len(classes)):\n",
    "    encoder[i] = classes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29756f972a1f4612bfdf20d4993585a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=350.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 5.75E-02\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#TWEEKED FASTAI\n",
    "from torch_lr_finder import LRFinder\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "classifier = model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=1e-7,weight_decay=1e-2)\n",
    "lr_finder = LRFinder(classifier, optimizer, criterion, device=device)\n",
    "lr_finder.range_test(train_iterator, end_lr=0.1, num_iter=350)\n",
    "#lr_finder.range_test(train_iterator, end_lr=100, num_iter=100)\n",
    "lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "lr_finder.reset() # to reset the model and optimizer to their in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#LESLIE SMITH\n",
    "from torch_lr_finder import LRFinder\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.1, weight_decay=1e-2)\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.001, weight_decay=1e-2)\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(train_iterator, val_loader=valid_iterator, end_lr=0.1, num_iter=40, step_mode=\"linear\")\n",
    "lr_finder.plot(log_lr=False)\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#FOUND_LR = 1e-3\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.0575)\n",
    "#optimizer = optim.SGD(model.parameters(), lr = 0.0264)\n",
    "#optimizer = optim.SGD(model.parameters(), lr = 0.000785)\n",
    "#optimizer = optim.SGD(model.parameters(), lr = 0.000000224)\n",
    "#optimizer = optim.SGD(model.parameters(), lr = 0.05)\n",
    "criterion = nn.CrossEntropyLoss() #our loss function (which will also apply the softmax activation function)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #to put our model and data on the GPU\n",
    "\n",
    "#model = model.to(device) # colocando el modelo en el device\n",
    "criterion = criterion.to(device) # colocando el criterio en el device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda,save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            # initialize weights to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # back prop\n",
    "            loss.backward()\n",
    "            \n",
    "            # grad\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Epoch %d, Batch %d loss: %.6f' %\n",
    "                  (epoch, batch_idx + 1, train_loss))\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            valid_loss_min = valid_loss\n",
    "              \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1 loss: 4.904364\n",
      "Epoch 1, Batch 101 loss: 4.890393\n",
      "Epoch 1, Batch 201 loss: 4.888535\n",
      "Epoch 1, Batch 301 loss: 4.886327\n",
      "Epoch 1, Batch 401 loss: 4.876527\n",
      "Epoch 1, Batch 501 loss: 4.866904\n",
      "Epoch 1, Batch 601 loss: 4.848593\n",
      "Epoch: 1 \tTraining Loss: 4.835149 \tValidation Loss: 4.651509\n",
      "Validation loss decreased (inf --> 4.651509).  Saving model ...\n",
      "Epoch 2, Batch 1 loss: 4.571523\n",
      "Epoch 2, Batch 101 loss: 4.671921\n",
      "Epoch 2, Batch 201 loss: 4.637563\n",
      "Epoch 2, Batch 301 loss: 4.626429\n",
      "Epoch 2, Batch 401 loss: 4.615039\n",
      "Epoch 2, Batch 501 loss: 4.602752\n",
      "Epoch 2, Batch 601 loss: 4.590831\n",
      "Epoch: 2 \tTraining Loss: 4.588365 \tValidation Loss: 4.459999\n",
      "Validation loss decreased (4.651509 --> 4.459999).  Saving model ...\n",
      "Epoch 3, Batch 1 loss: 4.136799\n",
      "Epoch 3, Batch 101 loss: 4.449347\n",
      "Epoch 3, Batch 201 loss: 4.433549\n",
      "Epoch 3, Batch 301 loss: 4.429031\n",
      "Epoch 3, Batch 401 loss: 4.425333\n",
      "Epoch 3, Batch 501 loss: 4.419917\n",
      "Epoch 3, Batch 601 loss: 4.414335\n",
      "Epoch: 3 \tTraining Loss: 4.414951 \tValidation Loss: 4.370277\n",
      "Validation loss decreased (4.459999 --> 4.370277).  Saving model ...\n",
      "Epoch 4, Batch 1 loss: 4.779905\n",
      "Epoch 4, Batch 101 loss: 4.300867\n",
      "Epoch 4, Batch 201 loss: 4.304698\n",
      "Epoch 4, Batch 301 loss: 4.305202\n",
      "Epoch 4, Batch 401 loss: 4.303131\n",
      "Epoch 4, Batch 501 loss: 4.303931\n",
      "Epoch 4, Batch 601 loss: 4.301130\n",
      "Epoch: 4 \tTraining Loss: 4.298063 \tValidation Loss: 4.245172\n",
      "Validation loss decreased (4.370277 --> 4.245172).  Saving model ...\n",
      "Epoch 5, Batch 1 loss: 4.741982\n",
      "Epoch 5, Batch 101 loss: 4.148298\n",
      "Epoch 5, Batch 201 loss: 4.153178\n",
      "Epoch 5, Batch 301 loss: 4.161056\n",
      "Epoch 5, Batch 401 loss: 4.160203\n",
      "Epoch 5, Batch 501 loss: 4.167650\n",
      "Epoch 5, Batch 601 loss: 4.161971\n",
      "Epoch: 5 \tTraining Loss: 4.157868 \tValidation Loss: 4.297966\n",
      "Epoch 6, Batch 1 loss: 4.437357\n",
      "Epoch 6, Batch 101 loss: 4.058296\n",
      "Epoch 6, Batch 201 loss: 4.064886\n",
      "Epoch 6, Batch 301 loss: 4.047567\n",
      "Epoch 6, Batch 401 loss: 4.049041\n",
      "Epoch 6, Batch 501 loss: 4.044951\n",
      "Epoch 6, Batch 601 loss: 4.039361\n",
      "Epoch: 6 \tTraining Loss: 4.044402 \tValidation Loss: 4.054244\n",
      "Validation loss decreased (4.245172 --> 4.054244).  Saving model ...\n",
      "Epoch 7, Batch 1 loss: 3.480497\n",
      "Epoch 7, Batch 101 loss: 3.909691\n",
      "Epoch 7, Batch 201 loss: 3.916683\n",
      "Epoch 7, Batch 301 loss: 3.888878\n",
      "Epoch 7, Batch 401 loss: 3.907250\n",
      "Epoch 7, Batch 501 loss: 3.914271\n",
      "Epoch 7, Batch 601 loss: 3.922054\n",
      "Epoch: 7 \tTraining Loss: 3.921006 \tValidation Loss: 4.032879\n",
      "Validation loss decreased (4.054244 --> 4.032879).  Saving model ...\n",
      "Epoch 8, Batch 1 loss: 4.877086\n",
      "Epoch 8, Batch 101 loss: 3.763122\n",
      "Epoch 8, Batch 201 loss: 3.816839\n",
      "Epoch 8, Batch 301 loss: 3.826893\n",
      "Epoch 8, Batch 401 loss: 3.820343\n",
      "Epoch 8, Batch 501 loss: 3.817511\n",
      "Epoch 8, Batch 601 loss: 3.822469\n",
      "Epoch: 8 \tTraining Loss: 3.815495 \tValidation Loss: 3.893092\n",
      "Validation loss decreased (4.032879 --> 3.893092).  Saving model ...\n",
      "Epoch 9, Batch 1 loss: 4.520104\n",
      "Epoch 9, Batch 101 loss: 3.704387\n",
      "Epoch 9, Batch 201 loss: 3.730163\n",
      "Epoch 9, Batch 301 loss: 3.732737\n",
      "Epoch 9, Batch 401 loss: 3.721135\n",
      "Epoch 9, Batch 501 loss: 3.716865\n",
      "Epoch 9, Batch 601 loss: 3.727503\n",
      "Epoch: 9 \tTraining Loss: 3.727233 \tValidation Loss: 3.785837\n",
      "Validation loss decreased (3.893092 --> 3.785837).  Saving model ...\n",
      "Epoch 10, Batch 1 loss: 3.638615\n",
      "Epoch 10, Batch 101 loss: 3.611552\n",
      "Epoch 10, Batch 201 loss: 3.620904\n",
      "Epoch 10, Batch 301 loss: 3.612272\n",
      "Epoch 10, Batch 401 loss: 3.630766\n",
      "Epoch 10, Batch 501 loss: 3.646071\n",
      "Epoch 10, Batch 601 loss: 3.640613\n",
      "Epoch: 10 \tTraining Loss: 3.647463 \tValidation Loss: 3.754906\n",
      "Validation loss decreased (3.785837 --> 3.754906).  Saving model ...\n",
      "Epoch 11, Batch 1 loss: 2.874635\n",
      "Epoch 11, Batch 101 loss: 3.491140\n",
      "Epoch 11, Batch 201 loss: 3.491477\n",
      "Epoch 11, Batch 301 loss: 3.495246\n",
      "Epoch 11, Batch 401 loss: 3.528197\n",
      "Epoch 11, Batch 501 loss: 3.530424\n",
      "Epoch 11, Batch 601 loss: 3.544006\n",
      "Epoch: 11 \tTraining Loss: 3.557694 \tValidation Loss: 3.682528\n",
      "Validation loss decreased (3.754906 --> 3.682528).  Saving model ...\n",
      "Epoch 12, Batch 1 loss: 3.611512\n",
      "Epoch 12, Batch 101 loss: 3.406857\n",
      "Epoch 12, Batch 201 loss: 3.397193\n",
      "Epoch 12, Batch 301 loss: 3.428219\n",
      "Epoch 12, Batch 401 loss: 3.437301\n",
      "Epoch 12, Batch 501 loss: 3.443012\n",
      "Epoch 12, Batch 601 loss: 3.450846\n",
      "Epoch: 12 \tTraining Loss: 3.449759 \tValidation Loss: 3.607445\n",
      "Validation loss decreased (3.682528 --> 3.607445).  Saving model ...\n",
      "Epoch 13, Batch 1 loss: 3.635633\n",
      "Epoch 13, Batch 101 loss: 3.386221\n",
      "Epoch 13, Batch 201 loss: 3.379840\n",
      "Epoch 13, Batch 301 loss: 3.393087\n",
      "Epoch 13, Batch 401 loss: 3.389827\n",
      "Epoch 13, Batch 501 loss: 3.394391\n",
      "Epoch 13, Batch 601 loss: 3.394603\n",
      "Epoch: 13 \tTraining Loss: 3.403927 \tValidation Loss: 3.737421\n",
      "Epoch 14, Batch 1 loss: 2.910635\n",
      "Epoch 14, Batch 101 loss: 3.257269\n",
      "Epoch 14, Batch 201 loss: 3.238601\n",
      "Epoch 14, Batch 301 loss: 3.267710\n",
      "Epoch 14, Batch 401 loss: 3.277245\n",
      "Epoch 14, Batch 501 loss: 3.294897\n",
      "Epoch 14, Batch 601 loss: 3.297395\n",
      "Epoch: 14 \tTraining Loss: 3.304343 \tValidation Loss: 3.545807\n",
      "Validation loss decreased (3.607445 --> 3.545807).  Saving model ...\n",
      "Epoch 15, Batch 1 loss: 3.044551\n",
      "Epoch 15, Batch 101 loss: 3.120058\n",
      "Epoch 15, Batch 201 loss: 3.144458\n",
      "Epoch 15, Batch 301 loss: 3.183858\n",
      "Epoch 15, Batch 401 loss: 3.219954\n",
      "Epoch 15, Batch 501 loss: 3.228485\n",
      "Epoch 15, Batch 601 loss: 3.250300\n",
      "Epoch: 15 \tTraining Loss: 3.253584 \tValidation Loss: 3.712519\n",
      "Epoch 16, Batch 1 loss: 2.800770\n",
      "Epoch 16, Batch 101 loss: 3.116303\n",
      "Epoch 16, Batch 201 loss: 3.161427\n",
      "Epoch 16, Batch 301 loss: 3.169045\n",
      "Epoch 16, Batch 401 loss: 3.180085\n",
      "Epoch 16, Batch 501 loss: 3.198555\n",
      "Epoch 16, Batch 601 loss: 3.191784\n",
      "Epoch: 16 \tTraining Loss: 3.203887 \tValidation Loss: 3.600992\n",
      "Epoch 17, Batch 1 loss: 2.873702\n",
      "Epoch 17, Batch 101 loss: 3.049052\n",
      "Epoch 17, Batch 201 loss: 3.089402\n",
      "Epoch 17, Batch 301 loss: 3.081803\n",
      "Epoch 17, Batch 401 loss: 3.091272\n",
      "Epoch 17, Batch 501 loss: 3.121188\n",
      "Epoch 17, Batch 601 loss: 3.131062\n",
      "Epoch: 17 \tTraining Loss: 3.144679 \tValidation Loss: 3.590099\n",
      "Epoch 18, Batch 1 loss: 2.217383\n",
      "Epoch 18, Batch 101 loss: 2.941587\n",
      "Epoch 18, Batch 201 loss: 3.014905\n",
      "Epoch 18, Batch 301 loss: 3.052046\n",
      "Epoch 18, Batch 401 loss: 3.081429\n",
      "Epoch 18, Batch 501 loss: 3.097320\n",
      "Epoch 18, Batch 601 loss: 3.089748\n",
      "Epoch: 18 \tTraining Loss: 3.097947 \tValidation Loss: 3.528983\n",
      "Validation loss decreased (3.545807 --> 3.528983).  Saving model ...\n",
      "Epoch 19, Batch 1 loss: 2.609641\n",
      "Epoch 19, Batch 101 loss: 2.939149\n",
      "Epoch 19, Batch 201 loss: 3.006209\n",
      "Epoch 19, Batch 301 loss: 2.996345\n",
      "Epoch 19, Batch 401 loss: 3.029637\n",
      "Epoch 19, Batch 501 loss: 3.070040\n",
      "Epoch 19, Batch 601 loss: 3.074174\n",
      "Epoch: 19 \tTraining Loss: 3.088359 \tValidation Loss: 3.560626\n",
      "Epoch 20, Batch 1 loss: 2.667499\n",
      "Epoch 20, Batch 101 loss: 2.918899\n",
      "Epoch 20, Batch 201 loss: 2.959476\n",
      "Epoch 20, Batch 301 loss: 2.992868\n",
      "Epoch 20, Batch 401 loss: 3.000175\n",
      "Epoch 20, Batch 501 loss: 2.999334\n",
      "Epoch 20, Batch 601 loss: 3.007568\n",
      "Epoch: 20 \tTraining Loss: 3.017878 \tValidation Loss: 3.514397\n",
      "Validation loss decreased (3.528983 --> 3.514397).  Saving model ...\n",
      "Epoch 21, Batch 1 loss: 1.972817\n",
      "Epoch 21, Batch 101 loss: 2.871012\n",
      "Epoch 21, Batch 201 loss: 2.851001\n",
      "Epoch 21, Batch 301 loss: 2.860554\n",
      "Epoch 21, Batch 401 loss: 2.915757\n",
      "Epoch 21, Batch 501 loss: 2.983633\n",
      "Epoch 21, Batch 601 loss: 3.015811\n",
      "Epoch: 21 \tTraining Loss: 3.032629 \tValidation Loss: 3.747496\n",
      "Epoch 22, Batch 1 loss: 3.829738\n",
      "Epoch 22, Batch 101 loss: 2.941631\n",
      "Epoch 22, Batch 201 loss: 2.913233\n",
      "Epoch 22, Batch 301 loss: 2.949061\n",
      "Epoch 22, Batch 401 loss: 2.980816\n",
      "Epoch 22, Batch 501 loss: 2.987297\n",
      "Epoch 22, Batch 601 loss: 2.999143\n",
      "Epoch: 22 \tTraining Loss: 3.009708 \tValidation Loss: 3.558126\n",
      "Epoch 23, Batch 1 loss: 1.427433\n",
      "Epoch 23, Batch 101 loss: 2.836224\n",
      "Epoch 23, Batch 201 loss: 2.890974\n",
      "Epoch 23, Batch 301 loss: 2.933810\n",
      "Epoch 23, Batch 401 loss: 2.950001\n",
      "Epoch 23, Batch 501 loss: 2.957468\n",
      "Epoch 23, Batch 601 loss: 2.961276\n",
      "Epoch: 23 \tTraining Loss: 2.948249 \tValidation Loss: 3.846868\n",
      "Epoch 24, Batch 1 loss: 3.864688\n",
      "Epoch 24, Batch 101 loss: 2.892037\n",
      "Epoch 24, Batch 201 loss: 2.834615\n",
      "Epoch 24, Batch 301 loss: 2.841044\n",
      "Epoch 24, Batch 401 loss: 2.874391\n",
      "Epoch 24, Batch 501 loss: 2.886539\n",
      "Epoch 24, Batch 601 loss: 2.907241\n",
      "Epoch: 24 \tTraining Loss: 2.925184 \tValidation Loss: 3.712290\n",
      "Epoch 25, Batch 1 loss: 2.445697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 101 loss: 2.829391\n",
      "Epoch 25, Batch 201 loss: 2.911821\n",
      "Epoch 25, Batch 301 loss: 2.881733\n",
      "Epoch 25, Batch 401 loss: 2.941877\n",
      "Epoch 25, Batch 501 loss: 2.947832\n",
      "Epoch 25, Batch 601 loss: 2.943673\n",
      "Epoch: 25 \tTraining Loss: 2.958499 \tValidation Loss: 3.750996\n",
      "Wall time: 37min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train the model\n",
    "model_scratch = train(25, loaders, model, optimizer,criterion, use_cuda, 'model_scratch.pt')\n",
    "#model_scratch = train(10, loaders, classifier, optimizer,criterion, True, 'model_scratch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scratch.load_state_dict(torch.load('model_scratch.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.566524\n",
      "\n",
      "\n",
      "Test Accuracy: 17% (143/836)\n"
     ]
    }
   ],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function    \n",
    "test(loaders, model, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders_transfer = loaders.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "## TODO: Specify model architecture \n",
    "#model_transfer = models.resnet50(pretrained=True)\n",
    "#model_transfer = models.resnet18(pretrained=True)\n",
    "model_transfer = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param in model_transfer.parameters(): #resnet\n",
    "    param.requires_grad = False #resnet\n",
    "\n",
    "#for the last layer\n",
    "model_transfer.fc = nn.Linear(2048, 133, bias=True) #resnet50\n",
    "\n",
    "fc_parameters = model_transfer.fc.parameters() #resnet\n",
    "\n",
    "for param in fc_parameters: #resnet\n",
    "    param.requires_grad = True #resnet\n",
    "    \n",
    "model_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use_cuda = torch.cuda.is_available()\n",
    "#print(use_cuda)\n",
    "#use_cuda = True\n",
    "# Reset last layer\n",
    "num_ftrs = model_transfer.fc.in_features\n",
    "model_transfer.fc = nn.Linear(num_ftrs, 133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_FEATURES = model_transfer.classifier[-1].in_features  #alexnet\n",
    "final_fc = nn.Linear(IN_FEATURES, 133) #alenext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #to put our model and data on the GPU\n",
    "\n",
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "optimizer_transfer = optim.SGD(model_transfer.parameters(), lr=0.001) #alexnet\n",
    "#optimizer_transfer = optim.SGD(model_transfer.fc.parameters(), lr=0.001) #resnet\n",
    "\n",
    "#model_transfer = model_transfer.to(device) # colocando el modelo en el device\n",
    "#criterion = criterion.to(device) # colocando el criterio en el device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_transfer.load_state_dict(torch.load('model_transfer.pt'))\n",
    "#model_transfer.load_state_dict(torch.load('model_scratch.pt'))\n",
    "model_transfer=train(5, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, True, 'model_transfer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transfer.load_state_dict(torch.load('model_transfer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# list of class names by index, i.e. a name can be accessed like class_names[0]\n",
    "class_names = [item[4:].replace(\"_\", \" \") for item in loaders_transfer['train'].dataset.classes]\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def load_input(img_path):    \n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    prediction = transforms.Compose([transforms.Resize(size=(224, 224)),\n",
    "                                     transforms.ToTensor(), \n",
    "                                     normalize])\n",
    "\n",
    "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
    "    result = prediction(image)[:3,:,:].unsqueeze(0)\n",
    "    return result\n",
    "\n",
    "def predict_breed_transfer(model, class_names, img_path):\n",
    "    # load the image and return the predicted breed\n",
    "    img = load_input(img_path)\n",
    "    model = model.cpu()\n",
    "    model.eval()\n",
    "    id = torch.argmax(model(img))\n",
    "    return class_names[id]\n",
    "\n",
    "for img_file in os.listdir('./images'):\n",
    "    img_path = os.path.join('./images', img_file)\n",
    "    predition = predict_breed_transfer(model_transfer, class_names, img_path)\n",
    "    print(\"image_name: {0}, \\t predition breed: {1}\".format(img_path, predition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_app(img_path):\n",
    "    ## handle cases for a human face, dog, and neither\n",
    "    img = Image.open(img_path)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    if dog_detector(img_path) is True:\n",
    "        prediction = predict_breed_transfer(model_transfer, class_names, img_path)\n",
    "        print(\"Dogs Detected!\\nIt looks like a {0}\".format(prediction))  \n",
    "    #elif face_detector(img_path) > 0:\n",
    "     #   prediction = predict_breed_transfer(model_transfer, class_names, img_path)\n",
    "      #  print(\"Hello, human!\\nIf you were a dog..You may look like a {0}\".format(prediction))\n",
    "    else:\n",
    "        prediction = predict_breed_transfer(model_transfer, class_names, img_path)\n",
    "        print(\"Hello, human!\\nIf you were a dog..You may look like a {0}\".format(prediction))\n",
    "\n",
    "for img_file in os.listdir('./images'):\n",
    "    img_path = os.path.join('./images', img_file)\n",
    "    run_app(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in np.hstack((human_files[:3], dog_files[:3])):\n",
    " #   run_app(file)\n",
    "for img_file in os.listdir('./images2'):\n",
    "    img_path = os.path.join('./images2', img_file)\n",
    "    run_app(img_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
